{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid_derivative(input):\n",
    "  return input * (1 - input)\n",
    "\n",
    "class Sigmoid:\n",
    "  def sigmoid(self,input):\n",
    "    return 1 / (1 + np.exp(-input))\n",
    "\n",
    "  def sigmoid_derivative(self,input):\n",
    "    return input * (1 - input)\n",
    "\n",
    "  def forward(self,input):\n",
    "    return self.sigmoid(input)\n",
    "\n",
    "  def backward(self,input):\n",
    "    return self.sigmoid_derivative(input)\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "  def relu(self,input):\n",
    "    return np.maximum(0,input)\n",
    "\n",
    "  def relu_derivative(self,input):\n",
    "    return np.where(input > 0, 1, 0)\n",
    "\n",
    "  def forward(self,input):\n",
    "    return self.relu(input)\n",
    "\n",
    "  def backward(self,input):\n",
    "    return self.relu_derivative(input)\n",
    "\n",
    "class MSE:\n",
    "  def mse(self,target,output):\n",
    "    return np.mean(0.5*(output - target)**2)\n",
    "\n",
    "  def mse_derivative(self,target,output):\n",
    "    return  target - output\n",
    "\n",
    "  def forward(self,target,output):\n",
    "    return self.mse(target,output)\n",
    "\n",
    "  def backward(self,target,output):\n",
    "    return self.mse_derivative(target,output)\n",
    "  \n",
    "class Linear:\n",
    "    def forward(self, input):\n",
    "        return input\n",
    "\n",
    "    def backward(self, input):\n",
    "        # Derivative of linear function is 1\n",
    "        return np.ones_like(input)\n",
    "\n",
    "class NeuralNetLayer:\n",
    "  def __init__(self,input_size,output_size):\n",
    "    self.input_size = input_size\n",
    "    self.output_size = output_size\n",
    "    self.weights = np.random.rand(input_size,output_size) * np.sqrt(2. / input_size)\n",
    "    self.bias = np.random.rand(1,output_size) * np.sqrt(2. / input_size)\n",
    "\n",
    "  def forward(self,input):\n",
    "    assert input.shape[-1] == self.input_size, \"Shape not same\"\n",
    "    output = (input @ self.weights) + self.bias\n",
    "    return output\n",
    "\n",
    "\n",
    "  def backward(self,input,error,lr):\n",
    "    grad = error @ self.weights.T \n",
    "    self.weights +=  lr * (input.T @ error)\n",
    "    self.bias += lr * error.mean(axis=0)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6580494166764389\n",
      "0.10331399201683275\n",
      "0.04109411379240492\n",
      "0.01958084072263737\n",
      "0.01030952355629797\n",
      "0.006077023391939303\n",
      "0.003952374451300624\n",
      "0.002794787831239696\n",
      "0.0021102089301917415\n",
      "0.0016552431261428518\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5416)\n",
    "x = np.random.randn(32,16)\n",
    "target = np.random.randn(32,1)\n",
    "layer1 = NeuralNetLayer(16,64)\n",
    "hidden_activation = ReLU()\n",
    "\n",
    "layer2 = NeuralNetLayer(64,1)\n",
    "output_activation = Linear()\n",
    "loss = MSE()\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    return x / np.linalg.norm(x,ord=1,axis=0)\n",
    "\n",
    "for i in range(10000):\n",
    "    # Forward Pass\n",
    "    hidden_layer = layer1.forward(x)\n",
    "    hidden_layer = normalize(hidden_layer)\n",
    "    hidden_layer = hidden_activation.forward(hidden_layer)\n",
    "    output_layer = layer2.forward(hidden_layer)\n",
    "    output_layer = output_activation.forward(output_layer)  # sometimes number goes big and sigmoid output becomes 1 so in that case use normalization\n",
    "    loss_output = loss.forward(target,output_layer)\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Bad Explanation Disclaimer!!:\n",
    "    (--- Iam a lazy person and I don't put much stress on my mind to find good words\n",
    "    thats why I make terrible explanations that nobody understands. Its all in the\n",
    "    mind that I can't explain in words cuz I dont have words. If the explanation\n",
    "    makes sense to  you  thats good and if dont go to hell. ---)\n",
    "\n",
    "    Take mean of gradient value of each sample in batch when using batch input,\n",
    "    Forward pass run in parallel for each sample in input. But only one backward\n",
    "    pass runs cuz changing every weight parallely (i.e parallel write to the same\n",
    "    weight matrix) does not make sense and also tensors or numpy array broadcast\n",
    "    does not allow this. Thats the whole idea of batch processing in neural networks.\n",
    "\n",
    "    Usually pytorch or other implementations take averge of loss and gradients but here\n",
    "    Iam not saving gradient separately so taking average of gradients across batch axis\n",
    "    and then update the weights\n",
    "    \n",
    "    Weights become too large that relu is simple return 1 for all weights and model is not\n",
    "    going anywhere to need to normalize. It will also prevent from weights overflow\n",
    "\n",
    "    Model is still not learning anything weights are large at initialization need to normalize\n",
    "    them while initializing   \n",
    "\n",
    "    Still not converging ah I found the problem my targets are in range of -1 to 1 and my \n",
    "    output layer activation function is sigmoid with range of 0 to 1 so its never gonna\n",
    "    converge need to change the activation function to linear\n",
    "    \n",
    "    Yay problem solved model is converging very fast and learning the input data distribution\n",
    "    .Now I can sleep peacefully\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Backward Pass\n",
    "    error = loss.backward(target,output_layer)\n",
    "    error = output_activation.backward(output_layer) * error\n",
    "    \n",
    "    error = layer2.backward(hidden_layer,error,0.01)\n",
    "    error = hidden_activation.backward(hidden_layer) * error\n",
    "    error = layer1.backward(x,error,0.01)\n",
    "    if i % 1000 == 0:\n",
    "      print(loss_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
